\documentclass[11pt]{article}

% allow UTF-8 input and proper font encoding
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% map the thin‐space and en‐dash so they don’t choke pdfLaTeX
\DeclareUnicodeCharacter{2009}{\,}% U+2009 THIN SPACE → \,
\DeclareUnicodeCharacter{2013}{\textendash}% U+2013 EN DASH → \textendash

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}

\definecolor{keyword}{RGB}{0,0,180}
\definecolor{comment}{RGB}{0,128,0}
\definecolor{string}{RGB}{163,21,21}

\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{keyword}\bfseries,
    commentstyle=\color{comment}\itshape,
    stringstyle=\color{string},
    showstringspaces=false,
    frame=single,
    breaklines=true,
    captionpos=b
}

\title{pAIcMan: CS262 Final Project\\Technical Write-Up}
\author{Matthew Vu, Pedro Garcia, Samuel Kim}
\date{\today}

\begin{document}
\maketitle

\section{Overview and Objectives}
\paragraph{Purpose.}
This project aimed to design and implement a real‐time, cross‐device Pacman game featuring networked multiplayer gameplay. The system utilizes a fault-tolerant Python server cluster, built with \texttt{pysyncobj} for Raft consensus, to maintain global game state and consistency. Lightweight Python clients, using Tkinter for the GUI, connect to the server to render the game and handle player input. Communication between clients and the server cluster is handled via gRPC, enabling potential interoperability between heterogeneous devices.

\paragraph{High-Level Goals.}
The primary objectives set for this project were:
\begin{enumerate}
  \item \emph{Networked Multiplayer:} Allow multiple players (one Pacman, one or more Ghosts) to participate in the same game session across different machines.
  \item \emph{Real-Time Synchronization:} Maintain a responsive gameplay experience with reasonably low latency between player actions and state updates, targeting a 60Hz refresh rate.
  \item \emph{Fault Tolerance:} Ensure the game state persists and gameplay can continue even if a minority of server replicas (up to $f$ in a $2f+1$ cluster) fail.
  \item \emph{Modularity:} Design the system with distinct client and server components communicating via a well-defined API (gRPC).
  \item \emph{Technology Exploration:} Gain practical experience with gRPC, consensus protocols (Raft via \texttt{pysyncobj}), and asynchronous programming in Python.
\end{enumerate}

\section{Technology Stack}
The following technologies and libraries form the foundation of the pAIcMan project:
\begin{itemize}
  \item \textbf{Programming Language:} Python 3.9+.
  \item \textbf{Core Libraries:} Built‐in modules like \texttt{asyncio}, \texttt{threading}, \texttt{queue}, \texttt{logging}, \texttt{argparse}.
  \item \textbf{Communication Protocol:} gRPC with Protocol Buffers (using \texttt{grpcio} and \texttt{grpcio-tools}) for defining the client-server API and handling remote procedure calls.
  \item \textbf{Consensus Algorithm:} Raft, implemented using the \texttt{pysyncobj} library, to provide fault tolerance for the server state.
  \item \textbf{Client GUI:} Tkinter, Python's standard GUI toolkit, used via the \texttt{helpers.graphicsDisplay} module adapted from the original Berkeley Pacman projects.
  \item \textbf{Logging:} Python's standard \texttt{logging} module for recording server and client events.
\end{itemize}

\section{System Architecture}
\subsection{gRPC API Definition (pacman.proto)}
The client-server interaction is defined by the Protocol Buffer specification in \texttt{pacman.proto}. This contract outlines the services, RPC methods, and message structures used for communication.

\paragraph{Key Services and RPCs.}
\begin{itemize}
    \item \textbf{Service \texttt{PacmanGame}:} Encapsulates the game-related operations.
    \item \textbf{RPC \texttt{CreateGame}:} Allows a client to request the creation of a new game session on the server cluster. Takes layout name and max players, returns game ID, assigned player ID, and layout name.
    \item \textbf{RPC \texttt{ListGames}:} Enables clients to query the server for currently active or waiting game sessions.
    \item \textbf{RPC \texttt{PlayGame}:} A bidirectional streaming RPC forming the core of gameplay. Clients send \texttt{PlayerAction} messages (containing intended direction), and the server streams back \texttt{GameState} updates.
\end{itemize}

\paragraph{Core Message Structures.}
\begin{lstlisting}[caption=Key Messages from \texttt{pacman.proto}]
// Represents a player's intended action (typically movement)
message PlayerAction {
  string player_id = 1;
  string game_id = 2;
  Direction direction = 3;
}

// Represents the state of a single agent (Pacman or Ghost)
message AgentState {
  string player_id = 1;
  Position position = 2;
  Direction direction = 3;
  AgentType agent_type = 4;
  int32 scared_timer = 5; // Relevant for ghosts
}

// Represents the complete state of a game session at a point in time
message GameState {
  string game_id = 1;
  repeated AgentState agents = 2;
  int32 score = 3;
  repeated Position food = 4;
  repeated Position capsules = 5;
  GameStatus status = 6;
  int64 state_version = 7; // Logical timestamp for ordering
}

// Other messages like Position, SessionInfo, CreateRequest, etc.
// Enums like Direction, AgentType, GameStatus
\end{lstlisting}

\subsection{Server Cluster \& Raft Consensus (PySyncObj)}
To achieve fault tolerance, the server logic is deployed as a cluster of identical replicas coordinated by the Raft consensus algorithm, facilitated by the \texttt{pysyncobj} library.

\begin{itemize}
  \item \textbf{Replication Model:} The server runs as multiple instances (typically 3 or 5 for 1 or 2-fault tolerance) of the \texttt{PacmanServicer} class. This class inherits from \texttt{pysyncobj.SyncObj}, providing the Raft machinery.
  \item \textbf{Replicated State Machine:} The core game state (active sessions, player assignments, positions, scores, food layout) is the state machine managed by Raft. Operations that modify this state (e.g., creating a game, player joining/leaving, processing player actions, updating game state) are implemented as methods within \texttt{PacmanServicer} or \texttt{GameSession} that are invoked via methods marked with the \texttt{@replicated} decorator in \texttt{PacmanServicer}. \texttt{PySyncObj} ensures these decorated methods are executed only after the corresponding command is committed in the Raft log across a majority of replicas.
  \item \textbf{Leader Role:} Raft elects a single leader among the replicas. Only the leader directly handles client requests that modify state (like \texttt{CreateGame} or receiving \texttt{PlayerAction}s within the \texttt{PlayGame} stream). The leader is also responsible for running the main game loop for active sessions.
  \item \textbf{Follower Role:} Follower replicas receive log entries from the leader via \texttt{pysyncobj}'s internal mechanisms and apply the committed commands to their local state machine, ensuring they stay synchronized. They typically do not interact directly with clients for write operations or game streaming.
  \item \textbf{State Persistence:} \texttt{PySyncObj} handles log persistence and snapshotting (configured via \texttt{SyncObjConf}) to allow replicas to recover their state after restarts.
\end{itemize}

\subsection{Server Game Loop and State Management}
Each active game session on the server is managed by a \texttt{GameSession} object, which runs its own update loop in a separate thread.
\begin{itemize}
    \item \textbf{Tick Rate:} The game loop aims for a 60Hz tick rate, controlled by calculating the elapsed time for each update cycle and sleeping for the remaining duration of the target interval (1/60th of a second).
    \item \textbf{State Updates:} Within the loop (\texttt{GameSession.update\_loop}), the server performs the following actions for each active game, but only if it is the current Raft leader:
    \begin{enumerate}
        \item \textbf{Process Actions:} Retrieves queued player actions (directions) that have been received via the \texttt{PlayGame} RPC and committed via Raft (using \texttt{\_replicated\_player\_action}).
        \item \textbf{Update Agent Positions:} Calculates the next position for each agent (Pacman and Ghosts) based on their current position, direction, and the layout's walls using logic from \texttt{helpers.game.Actions}.
        \item \textbf{Handle Collisions/Events:} Detects collisions with walls, food pellets, power capsules, and between Pacman and Ghosts. Updates the score, removes eaten food/capsules, manages scared ghost timers, and determines win/loss conditions.
        \item \textbf{Generate State Proto:} Creates a \texttt{pacman\_pb2.GameState} protobuf message representing the new state.
        \item \textbf{Replicate State:} Uses a \texttt{@replicated} method (\texttt{\_replicate\_game\_state}) to ensure the updated game state (score, agent positions, food, capsules, status) is consistently stored across the Raft cluster. This method includes the logical timestamp (\texttt{state\_version}).
        \item \textbf{Broadcast State:} Sends the newly generated \texttt{GameState} message to all connected clients in that session via their respective \texttt{PlayGame} streams.
    \end{enumerate}
    \item \textbf{No Server-Side AI:} Unlike some Pacman implementations, this version does not include server-controlled AI agents. All agents (Pacman and Ghosts) correspond to connected clients who provide input.
\end{itemize}

\subsection{Logical Clock for State Synchronization}
The server uses a simple timestamp-based mechanism to help clients order incoming game states, especially important given network latency variations.
\begin{itemize}
    \item \textbf{Timestamping:} The leader assigns a monotonically increasing timestamp (milliseconds since epoch, stored in \texttt{state\_version}) to each \texttt{GameState} it generates before replication and broadcast.
    \item \textbf{Client-Side Use:} The client's \texttt{GameStateAdapter} receives these states. While the primary consistency guarantee comes from Raft on the server-side, this timestamp allows the client to potentially identify and handle or discard out-of-order updates received over the network, ensuring smoother rendering based on the latest server-acknowledged state.
\end{itemize}

\section{Client Architecture}
\subsection{Initialization and Menus}
\begin{itemize}
  \item Upon launch, the client displays a main menu with options to create a new game, list existing games, or join an ongoing game session.
  \item The client uses gRPC stubs generated by \texttt{grpcio-tools} to call the \texttt{CreateGame} and \texttt{ListGames} RPCs.
\end{itemize}

\subsection{Client Architecture}
The client application is designed to be lightweight and responsive, focusing purely on rendering the game state received from the server cluster and transmitting player actions back. It connects to the server using gRPC, establishing a bidirectional stream for real-time gameplay communication via the `GameStream` RPC defined in our `.proto` file.

Key aspects of the client architecture include:
\begin{itemize}
    \item \textbf{Asynchronous Communication:} Leveraging Python's `asyncio` library and `grpc.aio`, the client handles network communication asynchronously. This ensures that the user interface remains responsive even while waiting for server updates or sending actions.
    \item \textbf{Server-权威 State Rendering:} The client does not perform any game logic simulation. It solely relies on the `GameState` messages received from the server cluster via the bidirectional stream. Upon receiving a new state, it updates the local display (using Pygame) to reflect the authoritative positions of Pac-Man, ghosts, pellets, and scores.
    \item \textbf{Action Transmission:} Player inputs (keyboard presses for movement) are translated into simple `PlayerAction` messages (e.g., specifying direction `UP`, `DOWN`, `LEFT`, `RIGHT`) and sent asynchronously to the server over the gRPC stream.
    \item \textbf{State Synchronization Awareness:} While the primary synchronization mechanism (Raft and logical clocks) resides on the server, the client implicitly benefits from it by receiving consistently ordered state updates.
    \item \textbf{Connection Management:} The client implements basic logic to handle connection establishment and graceful termination. It includes retry mechanisms with exponential backoff to manage transient network issues or temporary server unavailability during leader elections or node restarts, aiming to provide a smoother player experience.
\end{itemize}

\subsection{Deployment and Network Configuration}
Given the project's scope and focus on core distributed systems concepts, the deployment strategy centers on local network execution. The server cluster nodes and clients are intended to be run on separate machines within the same local network or on a single machine using different ports for simulation.

\begin{itemize}
    \item \textbf{Server Cluster Setup:} Each server node is configured with the addresses of its peers for Raft communication. A discovery mechanism or static configuration file (`config.json`) is used to initialize the cluster.
    \item \textbf{Client Connection:} Clients connect to a known entry point (e.g., one of the server node addresses). In a more robust setup, a load balancer or discovery service could abstract the specific server nodes from the client.
    \item \textbf{Network Considerations:} Running on a local network minimizes latency, which is crucial for a real-time game like Pac-Man. However, the design incorporates fault tolerance (via Raft) and client-side retries to handle potential packet loss or node failures, simulating challenges found in wider network deployments. Firewall configurations must allow traffic on the gRPC and Raft communication ports between the participating machines.
\end{itemize}
A cloud deployment (e.g., using Docker containers managed by Kubernetes on a platform like GCP, AWS, or Azure) would be a natural next step for broader accessibility but was outside the primary scope of this academic project.

\subsection{Testing Strategy}
A multi-layered testing approach was adopted to ensure correctness and robustness:
\begin{itemize}
    \item \textbf{Unit Tests:} Focused on isolated components:
        \begin{itemize}
            \item \textbf{gRPC Interface:} Testing serialization/deserialization of Protobuf messages (`GameState`, `PlayerAction`, etc.).
            \item \textbf{Raft (pysyncobj):} Leveraging the library's own tests and adding specific checks for our state machine application logic (applying game states).
            \item \textbf{Server Game Logic:} Testing core game functions (e.g., `move_pacman`, `move_ghost`, `check_collisions`, score updates) in isolation using mock objects and predefined scenarios.
            \item \textbf{State Synchronization:} Verifying the correct application and ordering of states based on logical clock values.
        \end{itemize}
    \item \textbf{Integration Tests:} Testing interactions between components:
        \begin{itemize}
            \item Client-Server Communication: Simulating a client connecting, sending actions, and receiving game state updates via the gRPC stream.
            \item Server Cluster Interaction: Launching a small cluster (3 nodes) and verifying leader election, state replication, and failover behavior under simulated node failures.
        \end{itemize}
    \item \textbf{Manual Testing:} Playing the game under various conditions (different layouts, numbers of ghosts, simulated network delays/failures) to identify usability issues and edge cases missed by automated tests.
    \item \textbf{Static Analysis:} Using tools like Ruff/Flake8 for linting and MyPy for static type checking to catch potential errors early and maintain code quality.
    \item \textbf{CI/CD (Conceptual):} Although not fully implemented with a dedicated CI server for this project, the structure facilitates automation. A pipeline (e.g., GitHub Actions) would ideally be configured to automatically run linters, type checkers, and unit/integration tests on each commit or pull request.
\end{itemize}

\subsection{Project Milestones}
The project development followed these approximate phases:
\begin{enumerate}
    \item \textbf{Foundation \& Setup:} Initial project structure, environment setup (Python, Pygame, gRPC tools), basic Pac-Man game logic (single-player, non-distributed).
    \item \textbf{gRPC API Definition:} Designing and implementing the `.proto` file for client-server communication (game creation, state streaming, actions). Generating initial Python stubs.
    \item \textbf{Basic Client-Server:} Implementing a single-server version where a client connects via gRPC, sends actions, and receives game state updates.
    \item \textbf{Raft Integration:} Integrating `pysyncobj` library, setting up the Raft cluster configuration, defining the replicated state machine to handle game state.
    \item \textbf{Distributed State Management:} Modifying the server logic to run within the Raft framework, ensuring state changes are proposed and committed through the consensus protocol. Implementing logical clocks for ordering.
    \item \textbf{Fault Tolerance \& Refinement:} Testing leader election, node failure scenarios, client reconnection logic. Refining the game loop and synchronization mechanisms.
    \item \textbf{Testing \& Documentation:} Writing unit and integration tests, performing manual testing, and drafting this technical write-up.
\end{enumerate}

\subsection{Lessons Learned}
This project provided valuable insights into building distributed, real-time applications:
\begin{itemize}
    \item \textbf{Complexity of Consensus:} Implementing and debugging distributed consensus (even using a library like `pysyncobj`) is challenging. Understanding the nuances of Raft (leader election, log replication, commit index) is crucial for correct operation and troubleshooting. State machine design must be deterministic and idempotent.
    \item \textbf{gRPC for Real-time Communication:} gRPC's bidirectional streaming is well-suited for real-time applications like games, providing an efficient and structured way to handle continuous updates. However, careful management of the stream lifecycle and error handling on both client and server is necessary.
    \item \textbf{State Synchronization Challenges:} Ensuring consistent game state across clients when the authoritative state resides on a potentially changing set of server nodes requires careful design. Logical clocks helped in managing causality, but the core consistency relies heavily on the underlying consensus protocol.
    \item \textbf{Asynchronous Programming:} `asyncio` is powerful for handling I/O-bound tasks like network communication but introduces its own complexities regarding task management, error propagation, and debugging compared to synchronous code.
    \item \textbf{Trade-offs in Fault Tolerance:} Achieving fault tolerance (e.g., 2-fault tolerance with a 5-node Raft cluster) comes at the cost of increased communication overhead and complexity compared to a single-server architecture. The choice depends heavily on the application's availability requirements.
    \item \textbf{Importance of Testing:} Thorough testing, especially integration testing that simulates network partitions or node failures, is indispensable for validating the correctness of distributed systems.
\end{itemize}
Future work could involve exploring alternative consensus protocols, implementing more sophisticated client-side prediction, deploying to a cloud environment, or enhancing the AI capabilities of the ghosts within the distributed framework.

\end{document}